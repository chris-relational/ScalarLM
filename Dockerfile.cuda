# The vLLM Dockerfile is used to construct vLLM image that can be directly used
# to run the OpenAI compatible server.

# Please update any changes made here to
# docs/source/dev/dockerfile/dockerfile.rst and
# docs/source/assets/dev/dockerfile-stages-dependency.png

ARG CUDA_VERSION=12.4.1
#################### BASE BUILD IMAGE ####################
# prepare basic build environment
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base
ARG CUDA_VERSION=12.4.1
ARG PYTHON_VERSION=3.12
ENV DEBIAN_FRONTEND=noninteractive

# Install Python and other dependencies
RUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \
    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \
    && apt-get update -y \
    && apt-get install -y ccache software-properties-common git curl sudo \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update -y \
    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \
    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \
    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \
    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \
    && python3 --version && python3 -m pip --version

# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519
# as it was causing spam when compiling the CUTLASS kernels
RUN apt-get install -y gcc-10 g++-10
RUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10
RUN <<EOF
RUN gcc --version

# Workaround for https://github.com/openai/triton/issues/2507 and
# https://github.com/pytorch/pytorch/issues/107960 -- hopefully
# this won't be needed for future versions of this docker image
# or future versions of triton.
RUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/

ARG INSTALL_ROOT=/app/cray

COPY ./infra/cray_infra/vllm ${INSTALL_ROOT}/vllm
COPY ./infra/setup.py ${INSTALL_ROOT}/setup.py

COPY ./infra/CMakeLists.txt ${INSTALL_ROOT}/CMakeLists.txt
COPY ./infra/cmake ${INSTALL_ROOT}/cmake
COPY ./infra/csrc ${INSTALL_ROOT}/csrc

COPY ./requirements.txt ${INSTALL_ROOT}/requirements.txt
COPY ./infra/requirements-common.txt ${INSTALL_ROOT}/requirements-common.txt
COPY ./infra/requirements-cuda.txt ${INSTALL_ROOT}/requirements-cuda.txt
COPY ./infra/requirements-vllm-build.txt ${INSTALL_ROOT}/requirements-vllm-build.txt

RUN pip install --no-compile --no-cache-dir -r ${INSTALL_ROOT}/requirements.txt
RUN pip install --no-compile --no-cache-dir -r ${INSTALL_ROOT}/requirements-cuda.txt

ARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'
ENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}
# Override the arch list for flash-attn to reduce the binary size
ARG vllm_fa_cmake_gpu_arches='80-real;90-real'
ENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}

RUN pip install --no-compile --no-cache-dir -r ${INSTALL_ROOT}/requirements-vllm-build.txt

WORKDIR ${INSTALL_ROOT}

# Build vllm python package
RUN \
    python3 ${INSTALL_ROOT}/setup.py bdist_wheel && \
    pip install ${INSTALL_ROOT}/dist/*.whl && \
    rm -rf ${INSTALL_ROOT}/dist

###############################################################################
# MAIN IMAGE
FROM vllm AS infra

RUN apt-get update -y  \
    && apt-get install -y slurm-wlm \
    && apt-get install -y mariadb-server build-essential munge libmunge-dev \
    && apt-get install -y less curl wget net-tools vim iputils-ping \
    && rm -rf /var/lib/apt/lists/*

# Copy slurm config templates
ENV PYTHONPATH="${PYTHONPATH}:${INSTALL_ROOT}/infra"
ENV PYTHONPATH="${PYTHONPATH}:${INSTALL_ROOT}/sdk"

ENV PATH=$PATH:${INSTALL_ROOT}/usr/bin
ENV SLURM_CONF=${INSTALL_ROOT}/infra/slurm_configs/slurm.conf

COPY ./infra ${INSTALL_ROOT}/infra
COPY ./sdk ${INSTALL_ROOT}/sdk
COPY ./test ${INSTALL_ROOT}/test
COPY ./models ${INSTALL_ROOT}/models
COPY ./cray ${INSTALL_ROOT}/cray
COPY ./ml ${INSTALL_ROOT}/ml
COPY ./scripts ${INSTALL_ROOT}/scripts

###############################################################################

